# **Table of Contents:**

1.0 [OpenAI API](#openai)
1. [Getting Started](#getting-started) 
2. [Understanding Message Objects and Roles](#message-objects) 
3. [First try to make a request to generate a problem](#first-try)

2.0 [Prompt Engineering – Optimizing Responses.](#prompt)
1. [Optimizing the First Written Code](#optimizations)

3.0 [Resources](#resources)

## **1.0 OpenAI API** {#openai}

**1.1 Getting started** {#getting-started} 

Step 1: Sign up for OpenAI API

- Visit the OpenAI website and sign up for an API key.

Step 2: Obtain API Key

- Once registered, access your OpenAI dashboard to find your API key.
- Keep your API key secure and avoid sharing it publicly.
- To keep our secure use environment variables.

Step 3: Install OpenAI Python Package

- Install the OpenAI Python package using pip:
- ```$pip install openai```

Step 4: Use OpenAI API in Python

Import the OpenAI module in your Python script or notebook.

import openai

Step 5: Set Up API Key

```py
openai.api\_key  = os.environ['OPENAI\_API\_KEY']
```

Step 6: Make an API call ( **Chat Completions API** ):

To make an API call, we will use **Chat Completions** API. This method is used to make an API call for generating chat-based completions.

```py
defget\_completion(_messages_,_model_="gpt-3.5-turbo"):

    response = client.chat.completions.create(

        _model_=model,

        _messages_=messages,

        _temperature_=0,

    )

    return response.choices[0].message.content
```

client.chat.completions.create parameters:

1. model: Specifies the GPT model to use (in this case, "gpt-3.5-turbo").
2. temperature: Set to 0 to make the output more deterministic (less random).
3. messages: The conversation history is provided as a list of message objects.

**1.2**  **Understanding Message Objects and Roles:** {#message-objects}

**Format of a Message Object** : A message object is a dictionary or JSON-like structure with two key-value pairs: 'role' and 'content'.

**Message Roles:**

'role': Specifies the role of the message in the conversation.

"system": They help set the tone, context, or specific behavior the model should exhibit during the conversation.

"user": represents inputs or queries from the end-user or person interacting with the model.

"assistant": responses generated by the model itself during the conversation. Usually used to give the model an example of a good response.

**Message Content:**

'content': Contains the text content of the message.

**Example messages List:**

```py
messages = [

    {"role":"system","content":"You are a helpful assistant."},

    {"role":"user","content":"Tell me a joke."},

    {"role":"assistant","content":"Sure, here's a joke: ..."},

    {"role":"user","content":"Tell me a fun fact."},
]
```

**1.3 First try to make a request to generate a problem.** {#first-try}

[**First try**](https://github.com/OsamaX01/Optimizing-OpenAI-aPI/blob/main/try_1.py)

**2.0 Prompt Engineering – Optimizing Responses** {#prompt}

Prompt engineering refers to the practice of carefully crafting input prompts to achieve desired outputs or behaviors from natural language processing (NLP) models, such as those provided by OpenAI, like GPT-3 or GPT-4. The goal is to optimize the model to generate responses that meet specific criteria, adhere to a particular style, or provide contextually relevant information.

**2.1**  **Optimizing The first written code:** {#optimizations}

1- Putting the system instructions in one system message gave much better results. [second [try](https://github.com/OsamaX01/Optimizing-OpenAI-aPI/blob/main/try_2.py)].

2- **Moderation (detect prompt injections)**: check that people are using the system responsibly and that they're not trying to abuse the system in some way. In This sense, we focus on prompts that ask to generate problems and handle irrelevant prompts. [Third [try](https://github.com/OsamaX01/Optimizing-OpenAI-aPI/blob/main/try_3.py)].

3- Tried the concept of " **Chain of Thought Reasoning"** , simply it's about dividing the system message into steps. In our case it gave worse results, I believe because our system message is big and has a lot of details [Forth [try](https://github.com/OsamaX01/Optimizing-OpenAI-aPI/blob/main/try_4_chain_of_thought.py)].

4-" **Chaining prompts"** : splitting complex tasks into a series of simpler subtasks by chaining multiple prompts (requests) one after another. [Fifth [try](https://github.com/OsamaX01/Optimizing-OpenAI-aPI/blob/main/try_5_chaining_prompts.py)].

5- Added **Assistant message:** it made the output format more consistent. [sixth [try](https://github.com/OsamaX01/Optimizing-OpenAI-aPI/blob/main/try_6_assistant.py)].

## **3.0 Resources** {#resources}

OpenAi official documentation:

[https://platform.openai.com/docs/guides/text-generation](https://platform.openai.com/docs/guides/text-generation).

DeepLearning.AI Building Systems with the ChatGPT API course:

[https://learn.deeplearning.ai/chatgpt-building-system](https://learn.deeplearning.ai/chatgpt-building-system)